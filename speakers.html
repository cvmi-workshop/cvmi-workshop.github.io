  <!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Computer Vision for Microscopy Image Analysis 2025</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/speakers.css">
</head>

<body>
    <div class="headBox">
   <!--     <div class="imgBox">
            <img class="logoBox" src="img/images/Picture2.png" alt="">

        </div>  -->

        <div class="navBox">
            <ul>
                <li>
                    <a href="./new.html"><i><strong class="new">New</strong></i></a></li>
                <li>
                    <a href="index.html"><strong>Welcome</strong></a></li>
                <li>
                    <a href="organizers.html"><strong>Organizers</strong></a></li>
                <li>
                    <a href="speakers.html"><strong>Invited Speakers</strong></a></li>
		<li>
                    <a href="program.html"><strong>Program</strong></a></li>
		<li>
                    <a href="registration.html"><strong>Registration</strong></a></li>
                <li>
                    <a href="venue.html"><strong>Attend</strong></a></li>
                <li>
                    <a href="sponsor.html"><strong>Sponsors</strong></a></li>
		<li>
                    <a href="jobs.html"><strong>Jobs</strong></a></li>
		<li>
                    <a href="comittee.html"><strong>Program Committee</strong></a></li>
                <li>
                    <a href="callForPaper.html"><strong>Call for Papers</strong></a></li>
                <li>
                    <a href="dates.html"><strong>Important Dates</strong></a></li>
                <li>
                    <a href="submission.html"><strong>Submission</strong></a></li>
                <li>
                    <a href="accepted.html"><strong>Accepted Papers</strong></a></li>
		<li>
                    <a href="spotlight.html"><strong>Works-in-Progress</strong></a></li>
		<li>
                    <a href="https://motchallenge.net/data/CTMC-v1/" target="_blank"><strong>CTMC Challenge</strong></a></li>

                <li>
                    <a href="contact.html"><strong>Contact</strong></a></li>
                <li>
                    <a href="pastcvmi.html"><strong>Past CVMIs</strong></a></li>
            </ul>
        </div>
    </div>

    <div class="contentBox">
        <div class="headImgBox">
            <img class="headImg" src="img/images/Picture25.png" alt="">
        </div>

        <div class="titleBox">
            <p style="font-size: 120%;font-weight: 700;">Invited Speakers</p>
        </div>
   
        <hr style="height:1px;border:none;border-top:1px solid #555555;">
        <div class="detailBox detailBoxSpeakers">

	  <div>
          <p>  <strong>Talk Title:</strong> 2025: the Digital Pathology Odyssey Continues</p>
             <strong>Start Time: </strong><span>8:40 AM CDT</span><br>		
		<strong>Speaker:</strong>
                <span>Inti Zlobec, Professor, University of Bern. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Inti.png" width="200" height="225">
		</center></p>
		<strong> Abstract:</strong> Pathology is going digital! On the one hand, digitization of glass slides and automated lab workflows are meant to improve efficiency and open the gateway to AI applications. On the other, pathologists are faced with interpreting images for diagnosis in a completely different way (on a monitor rather than microscope) yet still assuring the quality of their diagnoses with no change in turn-around-times. Is AI meant to help here? Generative AI and large language models (LLMs) have shown promising capabilities in delivering precise pathology diagnoses and suggesting effective treatment plans when paired with histological images. Foundational models have attracted attention for their ability to recognize complex tissue patterns and to be fine-tuned for enhanced accuracy in specific applications. With AI making headlines across the field, we would imagine that AI in pathology is ready for prime time. Is it? In today’s talk, we explore the answer to this question, taking a bird’s eye-view on where the community stands with the transition to digital pathology. We will then take a critical approach to the current market for AI applications and associated challenges. We will ask ourselves, which medical professionals do tissue-based AI algorithms benefit (pathologists or oncologists?), and how will pathologists and oncologists deal with the rise in increasingly complex biomarker algorithms? Indeed, pathologists seem ready to use AI tools based on hand-crafted features, leading to verifiable AI outputs, but what about those tools using end-to-end deep learning for predicting molecular aberrations, therapy response or prognosis? The computational companion diagnostic tests are starting to gain momentum, but how will pathologists trust tests whose results they cannot verify- should they?  Finally, we will look at where we go from here. The technologies are at our fingertips, including spatial transcriptomics and multiplex immunofluorescence, as well as vision-language models to name a few. To wrap up, we look at where the field is going and what we can expect in the near to mid- to long-term future.
		</p>	
		<strong>Speaker Bio:</strong>
                <span>Inti Zlobec holds the position of Professor (Extraordinarius) of Digital Pathology at the Institute of Pathology, University of Bern, Switzerland. She graduated with a PhD degree in Experimental Pathology, from McGill University, Montreal, Canada in 2007 before completing a post-doctoral fellowship at the Institute of Pathology, University Hospital Basel, where she conducted tissue-based research in the field of colorectal cancer using biostatistical models. After habilitating in 2010, she received a position at the Institute of Pathology, University of Bern, where she established and led the Translational Research Unit (TRU) and later the Tissue Bank Bern (TBB). Inti became Associate Professor in 2014. Now, she leads an inter-disciplinary research group of students and researchers using artificial intelligence and machine learning as tools to study pathology images along with other data types to discover and validate novel prognostic and predictive biomarkers for colorectal cancer patients. Inti is a member of the Executive Team of the Center for Artificial Intelligence in Medicine (CAIM) of the University of Bern, Co-Founder and President of the Swiss Consortium for Digital Pathology (SDiPath), Chair of the European Society of Pathology (ESP) Working Group Digital and Computational Pathology and Board Member of the European Society of Digital and Integrative Pathology (ESDIP).
                </span>
            </div> 

       <hr style="height:1px;border:none;border-top:1px solid #555555;" />   
       <br>
    
	    <div>

   	<p>  	<strong>Talk Title:</strong> The AI Revolution in Multimodal Radiology Informatics</p>
            	<strong>Start Time: </strong><span>9:20 AM CDT</span><br>
                <p><strong>Speaker:</strong> 	
                <span>Ron Summers, Senior Investigator, National Institute of Health</span><strong></strong> </p>
		<p><center>
		<img class="img-responsive" alt="" src="./img/images/ron.jpg" width="185" height="210">
		</center></p>
 	  	<strong> Abstract:</strong> Deep learning has enabled sophisticated AI analysis of radiology images, including CT, MRI, ultrasound, and radiography. Multimodal AI takes this sophistication to a new level. The incorporation of both clinical text and radiology images into multimodal models enables even more accurate predictions and expands the number of clinical use cases. Incorporation of multimodal imaging data sets is the next logical step. In this presentation, I will explore some of the latest developments in multimodal radiology AI with the goal to improve patient health.  
              	</p>
                <strong>Speaker Bio:</strong>
                <span>Dr. Summers received the BA degree in physics and the MD and PhD degrees in Medicine/Anatomy and Cell Biology from the University of Pennsylvania. He completed a medical internship at the Presbyterian-University of Pennsylvania Hospital, Philadelphia, PA, a radiology residency at the University of Michigan, Ann Arbor, MI, and an MRI fellowship at Duke University, Durham, NC. In 1994, he joined the Radiology and Imaging Sciences Department at the NIH Clinical Center in Bethesda, MD. where he is now a tenured Senior Investigator and Staff Radiologist. He is a Fellow of the Society of Abdominal Radiologists and of the American Institute for Medical and Biological Engineering (AIMBE). He directs the Imaging Biomarkers and Computer-Aided Diagnosis (CAD) Laboratory and is former and founding Chief of the NIH Clinical Image Processing Service. In 2000, he received the Presidential Early Career Award for Scientists and Engineers, presented by Dr. Neal Lane, President Clinton's science advisor. In 2012, he received the NIH Director's Award, presented by NIH Director Dr. Francis Collins. In 2017, he received the NIH Clinical Center Director's Award.

He has co-authored over 500 journal, review and conference proceedings articles and is a co-inventor on 14 patents. He is a member of the editorial boards of the Journal of Medical Imaging, Radiology: Artificial Intelligence and Academic Radiology and a past member of the editorial board of Radiology. He is a program committee member of the Computer-aided Diagnosis section of the annual SPIE Medical Imaging conference and was co-chair of the entire conference in 2018 and 2019. He was Program Co-Chair of the 2018 IEEE ISBI symposium.</span>
            </div>
	

	<hr style="height:1px;border:none;border-top:1px solid #555555;" />
        <br>  
	    <div>

 <!--	   <p>  <strong>Talk Title:</strong> Microscopy, foundation models, and the scaling hypothesis: a phenomenal step forward for image-based profiling</p>
		<strong>Start Time: </strong><span>10:30 AM CDT</span><br>	-->
        	<strong>Speaker:</strong>
		<span>Badri Roysam, Professor, University of Houston</span></p>
        	<p><center>
		<img class="img-responsive" alt="" src="./img/images/roysam.jpg"  width="230" height="200">
		</center></p>
            	<p>
    <!--		<strong> Abstract:</strong>The use of morphological profiles of cellular microscopy images is by now a widespread method of investigating the functional effects of perturbations and treatments on cellular models of disease, yet the insights gained from such analyses are only as good as the features extracted from these unstructured data. In this talk, I will describe how Recursion leveraged its phenomic datasets, computational resources, and a self-supervised learning objective to build Phenom-1, a foundation model of cellular morphology whose performance on downstream tasks like recall of known biological relationships appears to scale linearly in the logarithm of the total computational cost used to train it, a phenomenon known as the scaling hypothesis. A smaller version of this foundation model, called Phenom-Beta, was recently released under a non-commercial license on NVIDIA’s BioNemo platform. I will also briefly describe the role that foundation models like Phenom-1 play in a vision of the future of drug discovery, where AI agents generate and test hypotheses inferred from such models, and give a demo of LOWE, a first step towards this vision in which the cognitive capabilities of LLMs are leveraged to reason about and execute typical tasks involved in drug discovery: retrieval and analysis of data, design and execution of experiments, generation of compounds and prediction of their properties, etc.
          	</p>		-->
          	<strong>Speaker Bio: </strong>
		<span>Badri Roysam (Fellow IEEE, AIMBE) is the Hugh Roy and Lillie Cranz Cullen University Professor, and Chairman of the Electrical and Computer Engineering Department at the University of Houston (2010 – present). From 1989 to 2010, he was a Professor at Rensselaer Polytechnic Institute in Troy, New York, USA, where he directed the Rensselaer unit of the NSF Engineering Research (ERC) Center for Subsurface Sensing and Imaging Systems (CenSSIS ERC), and co-directed the Rensselaer Center for Open Source Software (RCOS) that was funded by a major alumnus gift. He received the Doctor of Science degree from Washington University, St. Louis, USA, in 1989. Earlier, he received his Bachelor’s degree in Electronics from the Indian Institute of Technology, Madras, India in 1984. Badri’s research is on the applications of multi-dimensional signal processing, machine learning, big-data bioinformatics, high-performance computing to problems in fundamental and clinical biomedicine. He collaborates with a diverse group of biologists, physicians, and imaging researchers. His work focuses on automated analysis of 2D/3D/4D/5D microscopy images from diverse applications including cancer immunotherapy, traumatic brain injury, retinal diseases, neural implants, learning and memory impairments, binge alcohol, tumor mapping, stem-cell biology, stroke research, and neurodegenerative diseases.</span>
	    </div>

	<br>
 	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>
	<br>

		<p>  <strong>Talk Title:</strong> Multimodal Generative AI for Precision Health</p>	
	 		<strong>Start Time: </strong><span>13:10 PM CDT</span><br>	
                <strong>Speaker:</strong>
                <span>Hoifung Poon, Ph.D., General Manager, Microsoft. </span><strong></strong> <br>
		<p><center>
		<img class="img-responsive" alt="" src="./img/images/Hoifung.jpg" width="200" height="200">
		</center></p>
		<strong> Abstract:</strong> The dream of precision health is to develop a data-driven, continuous learning system where new health information is instantly incorporated to optimize care delivery and accelerate biomedical discovery. The confluence of technological advances and social policies has led to rapid digitization of multimodal, longitudinal patient journeys, such as electronic medical records (EMRs), imaging, and multiomics. Our overarching research agenda lies in advancing multimodal generative AI for precision health, where we harness real-world data to pretrain powerful multimodal patient embedding, which can serve as digital twins for patients. This enables us to synthesize multimodal, longitudinal information for millions of cancer patients, and apply the population-scale real-world evidence to advancing precision oncology in deep partnerships with real-world stakeholders such as large health systems and life sciences companies.
            </p>  	
		<strong>Speaker Bio:</strong>
                <span>Hoifung Poon is General Manager at Health Futures in Microsoft Research and an affiliated faculty at the University of Washington Medical School. He leads biomedical AI research and incubation, with the overarching goal of structuring medical data to optimize delivery and accelerate discovery for precision health. His team and collaborators are among the first to explore large language models (LLMs) and multimodal generative AI in health applications, producing popular open-source foundation models such as PubMedBERT, BioGPT, BiomedCLIP, LLaVA-Med, BiomedParse, with tens of millions of downloads. His latest publication in Nature features GigaPath, the first whole-slide digital pathology foundation model pretrained on over one billion pathology image tiles. He has led successful research partnerships with large health providers and life science companies, creating AI systems in daily use for applications such as molecular tumor board and clinical trial matching. His prior work has been recognized with Best Paper Awards from premier AI venues such as NAACL, EMNLP, and UAI, and he was named the "Technology Champion" by the Puget Sound Business Journal in the 2024 Health Care Leadership Awards. He received his PhD in Computer Science and Engineering from the University of Washington, specializing in machine learning and NLP.
        <br>
		</span>  
	      </div>
	
	
            <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>
        <br>
		<p>  <strong>Talk Title:</strong> Generative Models of Cellular Organization</p>  
                <strong>Start Time: </strong><span>15:20 CDT</span><br>
		<strong>Speaker:</strong>
                <span>Greg Johnson, Ph.D, Research scientist, EvolutionaryScale. </span> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Greg.jpg" width="200" height="200">
		</center></p>
		<p>
        	<strong> Abstract:</strong> Understanding cells as integrated systems is a central challenge in modern biology. While fluorescence microscopy provides high-resolution views of specific subcellular structures, imaging many structures simultaneously or across time remains limited by phototoxicity, spectral overlap, and acquisition cost.
We address these limitations by learning joint representations across imaging modalities and subcellular structures. Our approach enables the generation of highly multiplexed, structure-specific visualizations from label-free inputs, bypassing the constraints of fluorescence labeling. We extend this framework to jointly model cell and nuclear morphology alongside subcellular localization, enabling conditional generation of realistic 3D images and the discovery of spatial associations between organelles.
By capturing these relationships, our method quantifies structural variability across cells and perturbation conditions. Finally, we outline extensions to incorporate additional data modalities—including perturbation metadata, single-cell transcriptomics, and genetic variation—to enable integrative modeling of cellular state and structure at scale.

            	</p>
		<strong>Speaker Bio:</strong>
                <span>Dr. Gregory R. Johnson is a research scientist at EvolutionaryScale, developing technology to model biological systems across diverse data types and scales. Previously, he co-founded NewLimit to pursue therapies targeting age-related diseases and built predictive models of cellular organization. His work spans machine learning applications in natural sciences and technology development across academia and industry. He earned his Ph.D. from Carnegie Mellon University, specializing in generative models to understand how cells are organized and respond to perturbations.</span>
            </div>


              <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	<br>      
	      <div>

		<p>  <strong>Talk Title:</strong> Cellular resilience: A new frontier for artificial intelligence-driven drug discovery</p>
                 <strong>Start Time: </strong><span>16:20 CDT</span><br>	
		<strong>Speaker:</strong>
                <span>Drew Linsley, Assistant Professor, Brown University. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Drew.jpeg" width="240" height="170">
		</center></p>
		<strong> Abstract:</strong> Neurodegenerative diseases often manifest without warning, yet their progression may be rooted in a fundamental cellular property: the gradual loss of resilience to everyday stressors that allows genetic predispositions to trigger disease onset and progression. On a molecular level, cellular resilience factors typically include proteostatic, metabolic, antioxidant and DNA repair pathways that capably compensate for pathological genetic mutations but that tend to degrade with aging. Despite growing recognition of cellular resilience as a promising therapeutic target, the field lacks systematic approaches to identify and validate interventions.
At Operant BioPharma, we have developed a novel robotic microscopy platform that can watch and control the biology of individual live cells at scale. In this talk, I will describe how we have used this platform to generate candidate therapeutic programs which reinforce cellular resilience in human Parkinson's disease models. Our approach represents a new paradigm for applying AI to complex progressive diseases by giving intelligent systems direct access to live cellular biology—enabling our systems to control, observe, and learn from how cells respond to perturbations in real-time at molecular, cellular, and network levels. This creates a fast and powerful feedback loop between AI-driven experimentation and biological discovery that can accelerate therapeutic development by discovering new programs that would be challenging to identify through other means.
</p>		
		<strong>Speaker Bio:</strong>
                <span>Dr. Linsley is an Assistant Professor (Research) in Computational Neuroscience and AI at Brown University. He previously did a postdoc with Thomas Serre, also at Brown. Before that, he received his PhD at Boston College with Sean MacEvoy, and BA at Hamilton College working with Jonathan Vaughan. He studies biological and artificial vision. He is Co-Founder and CEO at Operant Biopharma whose robotic microscope, powered by artificial intelligence, designs drugs by watching and controlling diseases over time.</span>
            </div>


	
            <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>
	<br>
  <!--    	    <p>  <strong>Talk Title:</strong> Microscopy, foundation models, and the scaling hypothesis: a phenomenal step forward for image-based profiling</p>
		<strong>Start Time: </strong><span>8:40 AM PDT</span><br>	
        	<strong>Speaker:</strong>
		<span>Berton Earnshaw, Ph.D., Machine Learning Fellow, Recursion.</span></p>
        	<p><center>
		<img class="img-responsive" alt="" src="./img/images/Berton.jpg"  width="210" height="210">
		</center></p>
            	<p>
    		<strong> Abstract:</strong>The use of morphological profiles of cellular microscopy images is by now a widespread method of investigating the functional effects of perturbations and treatments on cellular models of disease, yet the insights gained from such analyses are only as good as the features extracted from these unstructured data. In this talk, I will describe how Recursion leveraged its phenomic datasets, computational resources, and a self-supervised learning objective to build Phenom-1, a foundation model of cellular morphology whose performance on downstream tasks like recall of known biological relationships appears to scale linearly in the logarithm of the total computational cost used to train it, a phenomenon known as the scaling hypothesis. A smaller version of this foundation model, called Phenom-Beta, was recently released under a non-commercial license on NVIDIA’s BioNemo platform. I will also briefly describe the role that foundation models like Phenom-1 play in a vision of the future of drug discovery, where AI agents generate and test hypotheses inferred from such models, and give a demo of LOWE, a first step towards this vision in which the cognitive capabilities of LLMs are leveraged to reason about and execute typical tasks involved in drug discovery: retrieval and analysis of data, design and execution of experiments, generation of compounds and prediction of their properties, etc.
          	</p>		
          	<strong>Speaker Bio: </strong>
		<span>Berton Earnshaw is a Founding Fellow at Recursion, a leading clinical-stage TechBio company, and Scientific Director at Valence Labs, an AI research lab within Recursion whose mission is to industrialize scientific discovery to radically improve lives. Berton earned a PhD in mathematics from the University of Utah in its mathematical biology group, and was a postdoc at both the University of Utah and Michigan State University. Berton has worked in many scientific and leadership roles in industry, including CTO of Perfect Pitch (now Boomsourcing), Director of Data Science and Operations at Red Brain Labs (acquired by Savvysherpa), and Principal and Senior Scientist at Savvysherpa (acquired by UnitedHealth Group). While at Recursion, Berton has led the development and deployment of many of the machine learning capabilities employed in its drug discovery workflows, and currently directs multiple research programs across Recursion and Valence Labs.</span>
	    </div>
	<br>
 	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>
	<br>
          <p>  <strong>Talk Title:</strong> Predicting Patient Treatment Outcomes using (Diffusion) Generative Models </p>	
		<strong>Start Time: </strong><span>14:20 PDT</span><br>		
		<strong>Speaker:</strong>
                <span>Charlotte Bunne, Assistant Professor, EPFL. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Charlotte.jpg" width="200" height="200">
		</center></p>
		<strong> Abstract:</strong>As the smallest functioning living units, cells are key to understanding health and disease. To predict a patient’s responses to molecular drugs and design efficient treatments, it is vital to recover the underlying dynamics cells take upon administering a drug. Biologists have long sought to simulate the state and functioning of a cell in order to understand and control its core processes. In this talk, I will discuss how to use and design artificial intelligence tools combined with large biomedical datasets to infer such cellular behavior. I will cover our work on (diffusion and flow matching) generative models that robustly predict treatment responses of biopsied cells from metastatic melanoma patients. As integral part of an observational clinical cohort study, they are able to reveal otherwise hidden patterns of signaling pathway modulation associated with driver mutations and metastasis sites upon cancer treatments. Lastly, I will provide a perspective on how to develop biological foundation models and realize the vision of a virtual cell powered by artificial intelligence that will shape the future of treatment design and personalized therapies.</p>
		
		<strong>Speaker Bio:</strong>
                <span>Charlotte Bunne is an assistant professor at EPFL in the Computer Science and Life-Sciences Department. Before, she was a PostDoc at Genentech and Stanford and Before and completed a PhD in Computer Science at ETH Zurich working with Andreas Krause and Marco Cuturi. During her graduate studies, she was a visiting researcher at the Broad Institute of MIT and Harvard hosted by Anne Carpenter and Shantanu Singh and worked with Stefanie Jegelka at MIT. Her research aims to advance personalized medicine by utilizing machine learning and large-scale biomedical data. Charlotte has been a Fellow of the German National Academic Foundation and is a recipient of the ETH Medal.
                </span>
            </div>

   	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>

           <p>  <strong>Talk Title:</strong> Point-and-click: using microscopy images to guide spatial next generation sequencing measurements </p>
			<strong>Start Time: </strong><span>4:00 PM PDT</span><br>		
		<strong>Speaker:</strong>
                <span>Jocelyn Kishi, PhD, CEO & Co-Founder, Stealth TechBio Startup. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Josie.jpg" width="210" height="210">
		</center></p>
		<strong> Abstract:</strong> By using microscopy images of cells to guide multiplexed spatial indexing of sequencing reads, Light-Seq allows combined imaging and spatially resolved next generation sequencing (NGS) measurements to be captured from fixed biological samples. This is achieved through the combination of spatially targeted, rapid photocrosslinking of DNA barcodes onto complementary DNAs in situ with a one-step DNA stitching reaction to create pooled, spatially indexed sequencing libraries. Selection of cells can be done manually by pointing and clicking on the regions of interest (ROIs) to be sequenced, or automatically through the use of computer vision for cell type identification and segmentation. This foundational capability opens up a broad range of applications for multi-omic analysis unifying microscopy and NGS measurements from intact biological samples.</p>
		
		<strong>Speaker Bio:</strong>
                <span>Dr. Kishi’s research focuses on developing new methods for reading and writing DNA sequences, DNA computing and molecular robotics, and DNA-based imaging assays. My background in software engineering has allowed me to work on projects at the intersection of Computer Science and DNA Nanotechnology.
                </span>
            </div>

    	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>

           <p>  <strong>Talk Title:</strong> Enhancing SAM's Biomedical Image Analysis through Prompt-based Learning </p>
			<strong>Start Time: </strong><span>5:00 PM PDT</span><br>		
		<strong>Speaker:</strong>
                <span>Dong Xu, Curators’ Distinguished Professor, Uuniversity of Missouri, Columbia. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Dong.jpg" width="160" height="210">
		</center></p>
		<strong> Abstract:</strong> The Segment Anything Model (SAM), a foundational model trained on an extensive collection of images, presents many opportunities for diverse applications. For instance, we employed SAM in our biological pathway curation pipeline that synergizes image understanding and text mining techniques for deciphering gene relationships. SAM has proven highly efficient in recognizing pathway entities and their interconnections. However, SAM does not work well when applied to low-contrastive images directly. To counter this, we investigated prompt-based learning with SAM, specifically for identifying proteins from cryo-Electron Microscopy (cryo-EM) images. We trained a U-Net-based filter to adapt these grayscale cryo-EM images into RGB images suitable for SAM's input. We also trained continuous prompts and achieved state-of-the-art (SOTA) performance, even with a limited quantity of labeled data. The outcomes of our studies underscore the potential utilities of prompt-based learning on SAM for efficient biomedical image analyses.</p>
		
		<strong>Speaker Bio:</strong>
                <span>Dong Xu is Curators’ Distinguished Professor in the Department of Electrical Engineering and Computer Science, with appointments in the Christopher S. Bond Life Sciences Center and the Informatics Institute at the University of Missouri-Columbia. He obtained his Ph.D. from the University of Illinois, Urbana-Champaign in 1995 and did two years of postdoctoral work at the US National Cancer Institute. He was a Staff Scientist at Oak Ridge National Laboratory until 2003 before joining the University of Missouri, where he served as Department Chair of Computer Science during 2007-2016 and Director of Information Technology Program during 2017-2020. Over the past 30 years, he has conducted research in many areas of computational biology and bioinformatics, including single-cell data analysis, protein structure prediction and modeling, protein post-translational modifications, protein localization prediction, computational systems biology, biological information systems, and bioinformatics applications in human, microbes, and plants. His research since 2012 has focused on the interface between bioinformatics and deep learning. He has published more than 400 papers with more than 21,000 citations and an H-index of 73 according to Google Scholar. He was elected to the rank of American Association for the Advancement of Science (AAAS) Fellow in 2015 and American Institute for Medical and Biological Engineering (AIMBE) Fellow in 2020.
                </span>
            </div>   


        </div> -->
<br>

    </div> 
</body>

</html>