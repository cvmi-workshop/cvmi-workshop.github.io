<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Computer Vision for Microscopy Image Analysis 2021</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/speakers.css">
</head>

<body>
    <div class="headBox">
   <!--     <div class="imgBox">
            <img class="logoBox" src="img/images/Picture2.png" alt="">

        </div>  -->

        <div class="navBox">
            <ul>
                <li>
                    <a href="./new.html"><i><strong class="new">New</strong></i></a></li>
                <li>
                    <a href="index.html"><strong>Welcome</strong></a></li>
                <li>
                    <a href="organizers.html"><strong>Organizers</strong></a></li>
                <li>
                    <a href="speakers.html"><strong>Invited Speakers</strong></a></li>
		<li>
                    <a href="program.html"><strong>Program</strong></a></li>
		<li>
                    <a href="registration.html"><strong>Registration</strong></a></li>
                <li>
                    <a href="venue.html"><strong>Attend</strong></a></li>
                <li>
                    <a href="sponsor.html"><strong>Sponsors</strong></a></li>
		<li>
                    <a href="jobs.html"><strong>Jobs</strong></a></li>
		<li>
                    <a href="comittee.html"><strong>Program Committee</strong></a></li>
                <li>
                    <a href="callForPaper.html"><strong>Call for Papers</strong></a></li>
                <li>
                    <a href="dates.html"><strong>Important Dates</strong></a></li>
                <li>
                    <a href="submission.html"><strong>Submission</strong></a></li>
                <li>
                    <a href="accepted.html"><strong>Accepted Papers</strong></a></li>
		<li>
                    <a href="spotlight.html"><strong>Works-in-Progress</strong></a></li>
		<li>
                    <a href="https://motchallenge.net/data/CTMC-v1/" target="_blank"><strong>CTMC Challenge</strong></a></li>

                <li>
                    <a href="contact.html"><strong>Contact</strong></a></li>
                <li>
                    <a href="pastcvmi.html"><strong>Past CVMIs</strong></a></li>
            </ul>
        </div>
    </div>

    <div class="contentBox">
        <div class="headImgBox">
            <img class="headImg" src="img/images/Picture23.png" alt="">
        </div>

        <div class="titleBox">
            <p style="font-size: 120%;font-weight: 700;">Invited Speakers</p>
        </div>
   
<!--     <hr style="height:1px;border:none;border-top:1px solid #555555;">
        <div class="detailBox detailBoxSpeakers">

	  <div>
          <p>  <strong>Talk Title:</strong> Information in images for drug discovery: image-based profiling </p>
             <strong>Start Time: </strong><span>9:40 AM PDT</span><br>		
		<strong>Speaker:</strong>
                <span>Inti Zlobec, Professor, University of Bern. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Inti.png" width="200" height="225">
		</center></p>
		<strong> Abstract:</strong> Cell images contain a vast amount of quantifiable information about the status of the cell: for example, whether it is diseased, whether it is responding to a drug treatment, or whether a pathway has been disrupted by a genetic mutation. We extract hundreds of features of cells from images. Just like transcriptional profiling, the similarities and differences in the patterns of extracted features reveal connections among diseases, drugs, and genes. Improving this pipeline is an active area of research, from feature extraction to batch correction to quality control to assessing similarities.

We are harvesting similarities in image-based profiles to identify, at a single-cell level, how diseases, drugs, and genes affect cells, which can uncover small molecules’ mechanism of action, discover gene functions, predict assay outcomes, discover disease-associated phenotypes, identify the functional impact of disease-associated alleles, and find novel therapeutic candidates. As part of the JUMP-Cell Painting Consortium (Joint Undertaking for Morphological Profiling-Cell Painting) we are aiming to establish experimental and computational best practices for image-based profiling (https://jump-cellpainting.broadinstitute.org/results) and produce the world’s largest public Cell Painting gene/compound image resource, with 140,000 perturbations in five replicates, to be released November 2022. With these data and new technologies like Pooled Cell Painting and variants of the assay like LipocyteProfiler and CardioProfiler, we hope to bring drug discovery-accelerating applications to practice.
		</p>	
		<strong>Speaker Bio:</strong>
                <span>Inti Zlobec holds the position of Professor (Extraordinarius) of Digital Pathology at the Institute of Pathology, University of Bern, Switzerland. She graduated with a PhD degree in Experimental Pathology, from McGill University, Montreal, Canada in 2007 before completing a post-doctoral fellowship at the Institute of Pathology, University Hospital Basel, where she conducted tissue-based research in the field of colorectal cancer using biostatistical models. After habilitating in 2010, she received a position at the Institute of Pathology, University of Bern, where she established and led the Translational Research Unit (TRU) and later the Tissue Bank Bern (TBB). Inti Zlobec became Associate Professor in 2014. Now, she leads an inter-disciplinary research group of students and researchers using artificial intelligence and machine learning as tools to study pathology images along with other data types to discover and validate novel prognostic and predictive biomarkers for colorectal cancer patients. Inti Zlobec is a member of the Executive Team of the Center for Artificial Intelligence in Medicine (CAIM) of the University of Bern, Co-Founder and President of the Swiss Consortium for Digital Pathology (SDiPath) and Chair of the European Society of Pathology (ESP) Working Group IT.
                </span>
            </div> 
-->

       <hr style="height:1px;border:none;border-top:1px solid #555555;" />   
            <div>

<!--		<p>  <strong>Talk Title:</strong> Machine learning challenges in spatial single cell omics analysis </p>	-->
	 		<strong>Start Time: </strong><span>8:40 AM PDT</span><br>	
                <strong>Speaker:</strong>
                <span>Paula A. Marin Zapata, Data Scientist, Bayer Berlin. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Paula.jpeg" width="200" height="200">
		</center></p>
	<!--	<strong> Abstract:</strong> Methods for profiling RNA and protein expression in a spatially resolved manner are rapidly evolving, making it possible to comprehensively characterize cells and tissues in health and disease. The resulting large-scale and complex multimodal data sets raise interesting computational and machine learning challenges, from QC & storage to actual analysis. For instance what is the best description of a local cell neighborhood, how do we find such interesting ones and how are these differential across disease or other perturbations? And how can they be chained together to build a spatial human cell atlas?

Here, I will present approaches from the lab touching upon a few of these points. In particular, I will show how our recent toolbox Squidpy and the related SpatialData format support standard steps in analysis and visualization of spatial molecular data. I will then discuss recent approaches towards multimodal classification, learning cell cell communication and extension towards morphometric representations under perturbations using generative models.
            </p>  	-->
		<strong>Speaker Bio:</strong>
                <span>Paula is a data scientist from Bayer AG in Berlin. She obtained a PhD in biology from the German Cancer Research Center DKFZ, a MSc in Applied Mathematics from Eindhoven University of Technology, and a BSc in Biological Engineering from the National University of Colombia in Medellin. In 2017, she joined Bayer as a postdoctoral fellow, where she developed deep learning methods for phenotypic profiling in plant sciences and cellular images. Since 2019, she works at the Machine Learning Research group from Bayer R&D, where she focuses on image analysis applications to drug discovery.
            <br>
		</span>  
	      </div>
<br>
              <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	      <div>

		<p>  <strong>Talk Title:</strong> Building Large-Scale Foundation Models for Digital Pathology with Millions of Whole Slides and Multi-Modal Generative AI: from Virchow to PRISM </p>
                 <strong>Start Time: </strong><span>9:20 AM PDT</span><br>
		<strong>Speaker:</strong>
                <span>Siqi Liu, Ph.D., Director of AI Science, Paige AI. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Siqi.jpg" width="200" height="200">
		</center></p>
		<strong> Abstract:</strong>The application of artificial intelligence (AI) in precision medicine and decision support systems, particularly through the analysis of pathology images, holds immense potential to transform cancer diagnosis and treatment. However, implementing AI in computational pathology presents several challenges, including data heterogeneity, the need for large annotated datasets, huge compute costs, and the alignment between image tiles and whole slide information across multiple magnifications. To address these challenges, we propose a dual approach: (1) developing a family of large foundation models trained on millions of whole-slide images using self-supervised learning techniques to capture essential features and patterns across diverse pathology images; (2) pretraining an aggregator model that leverages vision and language through multimodal generative AI learning to incorporate clinical information typically reported in medical settings, thus enhancing the slide-level foundation model with richer contextual understanding. Our combined approach, developed through a collaboration between Paige and Microsoft, not only achieves state-of-the-art performance across various benchmarks but also demonstrates the generative AI’s potential to unlock new possibilities for next-generation computational pathology, promising more diverse applications and reduced development costs for better cancer diagnosis and treatment.</p>		
		<strong>Speaker Bio:</strong>
                <span>Siqi Liu is the Director of AI Science at Paige AI, based in New York City, US. Siqi oversees all AI product development and research initiatives at Paige. Paige AI is a leading company in digital pathology, leveraging advanced AI to revolutionize cancer diagnosis and treatment. Siqi's team aims to apply cutting-edge AI technology to real-world clinical and life sciences settings. Prior to joining Paige, Siqi worked at Siemens Healthineers in Princeton, US, as a staff scientist, where they applied AI to various radiology image modalities, including CT, MRI, and X-ray, Ultrasound, etc. Siqi earned a PhD in computer science from the University of Sydney, Australia in 2018, under the supervision of Weidong Cai.</span>
            </div>
<br>
       	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
            <div>

       <!--    <p>  	<strong>Talk Title:</strong> How Can Humans Learn from AI</p> -->
            	<strong>Start Time: </strong><span>11:15 AM PDT</span><br>	
                <p><strong>Speaker:</strong> 	
                <span>Stephen K. Burley, Uniersity Professor & Henry Rutgers Chair, Rutgers University </span><strong></strong> </p>
		<p><center>
		<img class="img-responsive" alt="" src="./img/images/Burley.jpg" width="165" height="210">
		</center></p>
      <!--    	<strong> Abstract:</strong> In traditional ML, models learn from hand-engineered features informed by existing domain knowledge. More recently, in the deep learning era, combining large-scale model architectures, compute, and datasets has enabled learning directly from raw data, often at the expense of human interpretability. In this talk, I'll discuss using deep learning to predict patient outcomes with interpretability methods to extract new knowledge that humans could learn and apply. This process is a natural next step in the evolution of applying ML to problems in medicine and science, moving from the use of ML to distill existing human knowledge to people using ML as a tool for knowledge discovery.           -->
            	</p>	
                <strong>Speaker Bio:</strong>
                <span>Stephen Burley currently serves as Henry Rutgers Chair and University Professor, Founding Director of the Institute for Quantitative Biomedicine, and Director of the RCSB Protein Data Bank at Rutgers, The State University of New Jersey. He is also a Member of the Rutgers Cancer Institute of New Jersey, where he Co-Leads the Cancer Pharmacology Research Program. Burley is an expert in structural biology, proteomics, bioinformatics, structure/fragment based drug discovery, and clinical medicine/oncology.

From 2008 to 2012, Burley was a Distinguished Lilly Research Scholar in Lilly Research Laboratories. Prior to joining Lilly, Burley served as the Chief Scientific Officer and Senior Vice President of SGX Pharmaceuticals, Inc., a publicly traded biotechnology company that was acquired by Lilly in 2008. Until 2002, Burley was the Richard M. and Isabel P. Furlaud Professor at The Rockefeller University and an Investigator in the Howard Hughes Medical Institute.

He has authored/coauthored more than 280 scholarly scientific articles. Following undergraduate training in applied mathematics and physics, Burley received an M.D. degree from Harvard Medical School in the joint Harvard-MIT Health Sciences and Technology Program and, as a Rhodes Scholar, received a D.Phil. in Structural Biology from Oxford University. He trained in internal medicine at the Brigham and Women's Hospital in Boston, and did post-doctoral work with Gregory A. Petsko at the Massachusetts Institute of Technology and Nobel Laureate William N. Lipscomb, Jr. at Harvard University. With William J. Rutter and others at the University of California San Francisco and Rockefeller, Burley co-founded Prospect Genomics, Inc., which was acquired by SGX in 2001. He is a Fellow of the Royal Society of Canada and of the New York Academy of Sciences, and recipient of a Doctor of Science (Honoris causa) from his alma mater the University of Western Ontario.</span>
            </div>
<br>
            <hr style="height:1px;border:none;border-top:1px solid #555555;" />
            <div>

	<!--       <p>  <strong>Talk Title:</strong> Decoding hidden signal from neurodegenerative drug discovery high-content screens  </p>  -->
               <strong>Start Time: </strong><span>1:00 PM PDT</span><br>	
		<strong>Speaker:</strong>
                <span>Hanchuang Peng, Ph.D, Allen Institute for Brain Science. </span> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/hanchuan.png" width="200" height="200">
		</center></p>
		<p>
   <!--     	<strong> Abstract:</strong> Alzheimer's disease is a complex and recalcitrant condition that has largely evaded traditional molecular drug discovery approaches. Phenotypic drug discovery using high-content cellular models with unbiased small molecule screening is promising but faces obstacles from subtle signal, artifacts, and non-specific visual markers. We propose two deep learning-based methods to overcome these challenges in large-scale cellular screens. First, we develop deep neural networks to generate missing fluorescence channel images from an Alzheimer's disease high-content screen (HCS), enabling the identification and prospective validation of overlooked but active small molecules. This is a unique application of generative images in drug discovery. Second, we introduce a learned biological landscape using deep metric learning to organize drug-like molecules by live-cell HCS images. Metric learning outperforms conventional image scoring and reveals previously hidden molecules that push diseased cells toward a healthy state as effectively as positive control compounds. These results indicate that a wealth of actionable biological information lies untapped but readily available in HCS datasets. -->
            	</p>
		<strong>Speaker Bio:</strong>
                <span>Hanchuan Peng joined the Allen Institute in 2012 to build a computational neuroanatomy and smart imaging group for the Institute’s new initiatives in neural coding and cell types. His current research focuses on bioimage analysis, large-scale informatics, machine learning, as well as computational biology. Before joining the Allen Institute, Peng was the head of a computational bioimage analysis lab at Howard Hughes Medical Institute, Janelia Farm Research Campus. His recent work includes developing novel algorithms for 3-D+ image analysis and data mining, building single-neuron whole-brain level 3-D digital atlases for model animals, and Vaa3D, which is a high-performance visualization-assisted analysis system for large 3-D+ biological and biomedical-image datasets. He is also the inventor of the widely cited minimum-redundant maximum-relevance (mRMR) feature selection algorithm in machine learning. Peng received his Ph.D. in biomedical engineering from Southeast University, China. He held postdoctoral positions at the Lawrence Berkeley National Laboratory at the University of California, Berkeley (computational biology, bioinformatics, and high-performance data mining with a particular focus on gene expression analysis) and at Johns Hopkins University Medical School (human brain imaging and analysis). He won several awards, including a Cozzarelli Prize (2013) for his collaborative research on dragonfly neurons, which “recognizes outstanding contributions to the scientific disciplines represented by the National Academy of Sciences (USA)”.</span>
            </div>
<br>
            <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>
      	    <p>  <strong>Talk Title:</strong> Microscopy, foundation models, and the scaling hypothesis: a phenomenal step forward for image-based profiling</p>
		<strong>Start Time: </strong><span>1:40 PM PDT</span><br>	
        	<p> <strong>Speaker:</strong>
		<span>Berton Earnshaw, Ph.D., Machine Learning Fellow, Recursion.</span></p>
        	<p><center>
		<img class="img-responsive" alt="" src="./img/images/Berton.jpg"  width="195" height="200">
		</center></p>
        
       		 <p>
    		<strong> Abstract:</strong>The use of morphological profiles of cellular microscopy images is by now a widespread method of investigating the functional effects of perturbations and treatments on cellular models of disease, yet the insights gained from such analyses are only as good as the features extracted from these unstructured data. In this talk, I will describe how Recursion leveraged its phenomic datasets, computational resources, and a self-supervised learning objective to build Phenom-1, a foundation model of cellular morphology whose performance on downstream tasks like recall of known biological relationships appears to scale linearly in the logarithm of the total computational cost used to train it, a phenomenon known as the scaling hypothesis. A smaller version of this foundation model, called Phenom-Beta, was recently released under a non-commercial license on NVIDIA’s BioNemo platform. I will also briefly describe the role that foundation models like Phenom-1 play in a vision of the future of drug discovery, where AI agents generate and test hypotheses inferred from such models, and give a demo of LOWE, a first step towards this vision in which the cognitive capabilities of LLMs are leveraged to reason about and execute typical tasks involved in drug discovery: retrieval and analysis of data, design and execution of experiments, generation of compounds and prediction of their properties, etc.
          	</p>		
          
		<strong>Speaker Bio: </strong>
		<span>Berton joined the founding team of Red Brain Labs as director of operations and data science, where he worked with current team members Ben Mabey and Mason Victors to optimize call centers using machine learning and simulation. In 2014, Red Brain Labs was acquired by Savvysherpa where, as a Senior Scientist and Principal, Berton led various research projects applying machine learning to insurance claims and medical records with the goal of improving healthcare outcomes. Before Red Brain Labs, Berton took some time off of academia following two post-docs at Utah and Michigan State University and co-founded the call center software company Perfect Pitch, where he led as CTO.

Berton earned his bachelor’s degree and master’s degree in mathematics from BYU, and in 2007 earned a Ph.D. in math from the University of Utah, where he designed biophysical models of protein trafficking at synapses during episodes of learning and memory formation. Outside of work, Berton loves traveling with his wife and raising their five children together, being outdoors, eating well, keeping his eye on financial markets and reading everything from fiction to philosophy to math and machine learning.</span>
<br>
<!--   	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>

           <p>  <strong>Talk Title:</strong> PLIP: Leveraging medical Twitter to build a visual–language foundation model for pathology AI </p>
			<strong>Start Time: </strong><span>3:30 PM PDT</span><br>		
		<strong>Speaker:</strong>
                <span>James Zou, Assistant Professor, Stanford University. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/JamesZou.jfif" width="210" height="210">
		</center></p>
		<strong> Abstract:</strong>The lack of annotated publicly available medical images is a major barrier for innovations. At the same time, many de-identified images and much knowledge are shared by clinicians on public forums such as medical Twitter. Here we harness these crowd platforms to curate OpenPath, a large dataset of 208,414 pathology images paired with natural language descriptions. This is the largest public dataset for pathology images annotated with natural text. We demonstrate the value of this resource by developing PLIP, a multimodal AI with both image and text understanding, which is trained on OpenPath. PLIP achieves state-of-the-art zero-shot and transfer learning performances for classifying new pathology images across diverse tasks. Moreover, PLIP enables users to retrieve similar cases by either image or natural language search, greatly facilitating knowledge sharing. Our approach demonstrates that publicly shared medical information is a tremendous resource that can be harnessed to advance biomedical AI.</p>
		
		<strong>Speaker Bio:</strong>
                <span>James Zou is an assistant professor of biomedical data science and, by courtesy, of CS and EE at Stanford University. Professor Zou develops novel machine and deep learning algorithms that have strong statistical guarantees; several of his methods are currently being used by biotech companies. He also works on questions important for the broader impacts of AI—e.g. interpretations, robustness, transparency—and on biotech and health applications. He has received several best paper awards, a Google Faculty Award, a Tencent AI award and is a Chan-Zuckerberg Investigator. He is also the faculty director of Stanford AI for Health program and is a member of the Stanford AI Lab. https://www.james-zou.com/.
                </span>
            </div>

    	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>

           <p>  <strong>Talk Title:</strong> Point-and-click: using microscopy images to guide spatial next generation sequencing measurements </p>
			<strong>Start Time: </strong><span>4:00 PM PDT</span><br>		
		<strong>Speaker:</strong>
                <span>Jocelyn Kishi, PhD, CEO & Co-Founder, Stealth TechBio Startup. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Josie.jpg" width="210" height="210">
		</center></p>
		<strong> Abstract:</strong> By using microscopy images of cells to guide multiplexed spatial indexing of sequencing reads, Light-Seq allows combined imaging and spatially resolved next generation sequencing (NGS) measurements to be captured from fixed biological samples. This is achieved through the combination of spatially targeted, rapid photocrosslinking of DNA barcodes onto complementary DNAs in situ with a one-step DNA stitching reaction to create pooled, spatially indexed sequencing libraries. Selection of cells can be done manually by pointing and clicking on the regions of interest (ROIs) to be sequenced, or automatically through the use of computer vision for cell type identification and segmentation. This foundational capability opens up a broad range of applications for multi-omic analysis unifying microscopy and NGS measurements from intact biological samples.</p>
		
		<strong>Speaker Bio:</strong>
                <span>Dr. Kishi’s research focuses on developing new methods for reading and writing DNA sequences, DNA computing and molecular robotics, and DNA-based imaging assays. My background in software engineering has allowed me to work on projects at the intersection of Computer Science and DNA Nanotechnology.
                </span>
            </div>



    	    <hr style="height:1px;border:none;border-top:1px solid #555555;" />
	    <div>

           <p>  <strong>Talk Title:</strong> Enhancing SAM's Biomedical Image Analysis through Prompt-based Learning </p>
			<strong>Start Time: </strong><span>5:00 PM PDT</span><br>		
		<strong>Speaker:</strong>
                <span>Dong Xu, Curators’ Distinguished Professor, Uuniversity of Missouri, Columbia. </span><strong></strong> <br>
                <p><center>
		<img class="img-responsive" alt="" src="./img/images/Dong.jpg" width="160" height="210">
		</center></p>
		<strong> Abstract:</strong> The Segment Anything Model (SAM), a foundational model trained on an extensive collection of images, presents many opportunities for diverse applications. For instance, we employed SAM in our biological pathway curation pipeline that synergizes image understanding and text mining techniques for deciphering gene relationships. SAM has proven highly efficient in recognizing pathway entities and their interconnections. However, SAM does not work well when applied to low-contrastive images directly. To counter this, we investigated prompt-based learning with SAM, specifically for identifying proteins from cryo-Electron Microscopy (cryo-EM) images. We trained a U-Net-based filter to adapt these grayscale cryo-EM images into RGB images suitable for SAM's input. We also trained continuous prompts and achieved state-of-the-art (SOTA) performance, even with a limited quantity of labeled data. The outcomes of our studies underscore the potential utilities of prompt-based learning on SAM for efficient biomedical image analyses.</p>
		
		<strong>Speaker Bio:</strong>
                <span>Dong Xu is Curators’ Distinguished Professor in the Department of Electrical Engineering and Computer Science, with appointments in the Christopher S. Bond Life Sciences Center and the Informatics Institute at the University of Missouri-Columbia. He obtained his Ph.D. from the University of Illinois, Urbana-Champaign in 1995 and did two years of postdoctoral work at the US National Cancer Institute. He was a Staff Scientist at Oak Ridge National Laboratory until 2003 before joining the University of Missouri, where he served as Department Chair of Computer Science during 2007-2016 and Director of Information Technology Program during 2017-2020. Over the past 30 years, he has conducted research in many areas of computational biology and bioinformatics, including single-cell data analysis, protein structure prediction and modeling, protein post-translational modifications, protein localization prediction, computational systems biology, biological information systems, and bioinformatics applications in human, microbes, and plants. His research since 2012 has focused on the interface between bioinformatics and deep learning. He has published more than 400 papers with more than 21,000 citations and an H-index of 73 according to Google Scholar. He was elected to the rank of American Association for the Advancement of Science (AAAS) Fellow in 2015 and American Institute for Medical and Biological Engineering (AIMBE) Fellow in 2020.
                </span>
            </div>   


        </div> -->
<br>

    </div> 
</body>

</html>