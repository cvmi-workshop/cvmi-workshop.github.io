<?xml version="1.0"?>
<?mso-application progid="Excel.Sheet"?>
<Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet"
 xmlns:o="urn:schemas-microsoft-com:office:office"
 xmlns:x="urn:schemas-microsoft-com:office:excel"
 xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet"
 xmlns:html="http://www.w3.org/TR/REC-html40">
 <DocumentProperties xmlns="urn:schemas-microsoft-com:office:office">
  <Author>Mei Chen</Author>
  <LastAuthor>Mei Chen</LastAuthor>
  <Created>2025-04-09T18:24:39Z</Created>
  <Version>16.00</Version>
 </DocumentProperties>
 <OfficeDocumentSettings xmlns="urn:schemas-microsoft-com:office:office">
  <AllowPNG/>
 </OfficeDocumentSettings>
 <ExcelWorkbook xmlns="urn:schemas-microsoft-com:office:excel">
  <WindowHeight>3807</WindowHeight>
  <WindowWidth>9487</WindowWidth>
  <WindowTopX>32767</WindowTopX>
  <WindowTopY>32767</WindowTopY>
  <ProtectStructure>False</ProtectStructure>
  <ProtectWindows>False</ProtectWindows>
 </ExcelWorkbook>
 <Styles>
  <Style ss:ID="Default" ss:Name="Normal">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font ss:FontName="Aptos Narrow" x:Family="Swiss" ss:Size="11"
    ss:Color="#000000"/>
   <Interior/>
   <NumberFormat/>
   <Protection/>
  </Style>
  <Style ss:ID="s62">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font ss:FontName="Arial" x:Family="Swiss" ss:Bold="1"/>
   <Interior/>
   <NumberFormat/>
   <Protection/>
  </Style>
  <Style ss:ID="s63">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font ss:FontName="Aptos Narrow" x:Family="Swiss" ss:Size="11"
    ss:Color="#000000"/>
   <Interior/>
   <NumberFormat ss:Format="0%"/>
   <Protection/>
  </Style>
 </Styles>
 <Worksheet ss:Name="CVMI2025">
  <Table ss:ExpandedColumnCount="38" ss:ExpandedRowCount="13" x:FullColumns="1"
   x:FullRows="1" ss:DefaultColumnWidth="50.666666666666671"
   ss:DefaultRowHeight="14.333333333333332">
   <Row>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
   </Row>
   <Row ss:Index="3">
    <Cell ss:StyleID="s62"><Data ss:Type="String">Paper ID</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Created</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Last Modified</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Paper Title</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Abstract</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Primary Contact Author Name</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Primary Contact Author Email</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Authors</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Author Names</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Author Emails</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Track Name</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Primary Subject Area</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Secondary Subject Areas</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Conflicts</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Domains</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Assigned</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Completed</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">% Completed</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Bids</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Discussion</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Status</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Requested For Author Feedback</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Author Feedback Submitted?</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Requested For Camera Ready</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Camera Ready Submitted?</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Requested For Presentation</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Files</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Number of Files</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Supplementary Files</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Number of Supplementary Files</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Reviewers</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Reviewer Emails</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">MetaReviewers</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">MetaReviewer Emails</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">SeniorMetaReviewers</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">SeniorMetaReviewerEmails</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Q1 (Guidelines Confirmed)</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Q2 (Keywords)</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">10</Data></Cell>
    <Cell><Data ss:Type="String">3/9/2025 2:45:00 PM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">3/31/2025 4:45:59 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">Fast Sphericity and Roundness approximation in 2D and 3D using Local Thickness</Data></Cell>
    <Cell><Data ss:Type="String">Sphericity and roundness are fundamental measures used for assessing object uniformity in 2D and 3D images. However, using their strict definition makes computation costly. As both 2D and 3D microscopy imaging datasets grow larger, there is an increased demand for efficient algorithms that can quantify multiple objects in large volumes. We propose a novel approach for extracting sphericity and roundness based on the output of a local thickness algorithm. For sphericity, we simplify the surface area computation by modeling objects as spheroids/ellipses of varying lengths and widths of mean local thickness. For roundness, we avoid a complex corner curvature determination process by approximating it with local thickness values on the contour/surface of the object. The resulting methods provide an accurate representation of the exact measures while being significantly faster than their existing implementations.</Data></Cell>
    <Cell><Data ss:Type="String">Pawel Pieta</Data></Cell>
    <Cell><Data ss:Type="String">papi@dtu.dk</Data></Cell>
    <Cell><Data ss:Type="String">Pawel Pieta (Technical University of Denmark)*; Peter Winkel Rasmussen (Technical University of Denmark); Anders Bjorholm Dahl (Technical University of Denmark); Anders Nymark Christensen (Technical University of Denmark)</Data></Cell>
    <Cell><Data ss:Type="String">Pieta, Pawel*; Rasmussen, Peter Winkel; Dahl, Anders Bjorholm; Christensen, Anders Nymark</Data></Cell>
    <Cell><Data ss:Type="String">papi@dtu.dk*; pwra@dtu.dk; abda@dtu.dk; anym@dtu.dk</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String">dtu.dk</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">_CVMI_2025_ID_10.pdf (5,814,316 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Alexandr A. Kalinin (Broad Institute of MIT and Harvard); Hritam Basak (Stony Brook University); Jamalia  Sultana  (Stony Brook University)</Data></Cell>
    <Cell><Data ss:Type="String">akalinin@broadinstitute.org; hbasak@cs.stonybrook.edu; jsultana@cs.stonybrook.edu</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">shape analysis, roundness, sphericity</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">11</Data></Cell>
    <Cell><Data ss:Type="String">3/11/2025 2:26:21 PM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">4/1/2025 6:52:44 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks</Data></Cell>
    <Cell><Data ss:Type="String"> We present SAM4EM, a novel approach for 3D segmentation of complex neural structures in electron microscopy (EM) data by leveraging the Segment Anything Model (SAM) alongside advanced fine-tuning strategies. Our contributions include the development of a prompt-free adapter for SAM using two stage mask decoding to automatically generate prompt embeddings, a dual-stage fine-tuning method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with limited annotated data, and a 3D memory attention mechanism to ensure segmentation consistency across 3D stacks.  We further release a unique benchmark dataset for the segmentation of astrocytic processes and synapses. We evaluated our method on challenging neuroscience segmentation benchmarks, specifically targeting mitochondria, glia, and synapses, with significant accuracy improvements over state-of-the-art (SOTA) methods, including recent SAM-based adapters developed for the medical domain and other vision transformer-based approaches. Experimental results indicate that our approach outperforms existing solutions in the segmentation of complex processes like glia and post-synaptic densities.</Data></Cell>
    <Cell><Data ss:Type="String">Marco Agus</Data></Cell>
    <Cell><Data ss:Type="String">magus@hbku.edu.qa</Data></Cell>
    <Cell><Data ss:Type="String">Uzair Shah (HBKU); Marco Agus (HBKU)*; Daniya Boges (KAUST); Vanessa Chappini (University of Turin); Mahmood Alzubaidi (HBKU); Jens Schneider (HBKU); Markus Hadwiger (KAUST); Pierre Magistretti (KAUST); Mowafa Househ (HBKU); Corrado Cali (University of Turin)</Data></Cell>
    <Cell><Data ss:Type="String">Shah, Uzair; Agus, Marco*; Boges, Daniya; Chappini, Vanessa; Alzubaidi, Mahmood; Schneider, Jens; Hadwiger, Markus; Magistretti, Pierre; Househ, Mowafa; Cali, Corrado</Data></Cell>
    <Cell><Data ss:Type="String">uzsh31989@hbku.edu.qa; magus@hbku.edu.qa*; daniya.boges@kaust.edu.sa; vanessa.chiappini@unito.it; malzubaidi@hbku.edu.qa; jeschneider@hbku.edu.qa; markus.hadwiger@kaust.edu.sa; pierre.magistretti@kaust.edu.sa; mhouseh@hbku.edu.qa; corrado.cali@unito.it</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">crs4.it;kaust.edu.sa;tum.edu;hbku.edu.qa;unito.it</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">SAM4EM_CVPR2025.pdf (6,486,196 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Alexandr A. Kalinin (Broad Institute of MIT and Harvard); Dong Yang (NVIDIA Corporation); Jignesh Chowdary Gutta (Stony Brook University)</Data></Cell>
    <Cell><Data ss:Type="String">akalinin@broadinstitute.org; dongy@nvidia.com; jgutta@cs.stonybrook.edu</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">neuroscience,3D segmentation, serial section electron microscopy, glia segmentation, segment anything model</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">13</Data></Cell>
    <Cell><Data ss:Type="String">3/12/2025 10:21:32 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">4/2/2025 7:08:00 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images</Data></Cell>
    <Cell><Data ss:Type="String">The Segment Anything Model (SAM) is widely used for segmenting a diverse range of objects in natural images from simple user prompts like points or bounding boxes. However, SAM’s performance decreases substantially when applied to non-natural domains like microscopic imaging. Furthermore, due to SAM’s interactive design, it requires a precise prompt for each image and object, which is unfeasible in many automated biomedical applications. Previous solutions adapt SAM by training millions of parameters via fine-tuning large parts of the model or of adapter layers. In contrast, we show that as little as 2,048 additional parameters are sufficient for turning SAM into a use-case specialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM) method uses prompt-tuning, a parameter-efficient fine-tuning technique, to adapt SAM for a specific task. We validate the performance of our approach on multiple microscopic and one medical dataset.&#10;Our results show that prompt-tuning only SAM’s mask decoder already leads to a performance on-par with state-of-the-art techniques while requiring roughly 2,000x less trainable parameters. For addressing domain gaps, we find that additionally prompt-tuning SAM’s image encoder is beneficial, further improving segmentation accuracy by up to 18% over state-of-the-art results. Since PTSAM can be reliably trained with as little as 16 annotated images, we find it particularly helpful for applications with limited training data and domain shifts.</Data></Cell>
    <Cell><Data ss:Type="String">Björn Barz</Data></Cell>
    <Cell><Data ss:Type="String">bjoern.barz@zeiss.com</Data></Cell>
    <Cell><Data ss:Type="String">Tristan Piater (Carl Zeiss AG); Björn Barz (Carl Zeiss AG)*; Alexander Freytag (Carl Zeiss AG)</Data></Cell>
    <Cell><Data ss:Type="String">Piater, Tristan; Barz, Björn*; Freytag, Alexander</Data></Cell>
    <Cell><Data ss:Type="String">tristan.piater@zeiss.com; bjoern.barz@zeiss.com*; alexander.freytag@zeiss.com</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">zeiss.com;uni-jena.de</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">ptsam_micro_cvpr_review.pdf (738,410 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Jay Patravali (Microsoft); Peng Liu (Boston college); Tong Zhang (Peng Cheng Labortory)</Data></Cell>
    <Cell><Data ss:Type="String">jaypatravali@gmail.com; liupen@bc.edu; zhangt02@pcl.ac.cn</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">SAM&#10;Segment Anything&#10;Image Segmentation&#10;Semantic Segmentation&#10;Vision Foundation Models&#10;Parameter-efficient Fine-Tuning&#10;Prompt Tuning&#10;Visual Prompt Tuning&#10;Microscopy Image Analysis&#10;Medical Image Analysis</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">19</Data></Cell>
    <Cell><Data ss:Type="String">3/15/2025 6:48:23 PM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">3/31/2025 4:47:03 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">A Novel 3D Decoder with Weighted and Learnable Triple Attention for 3D Microscopy Image Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Deep neural networks are the backbone of 3D medical image segmentation architectures, showcasing exceptional application capabilities. However, their increasing model size and computational demands present significant challenges for deployment in real-world medical applications.  To further advance this area, we introduce the Weighted and Learnable Triple Attention Network (WLTA-Net), a high-performance and efficient model. The WLTA-Net encoder consists of a Swin Transformer, where the multi-scale outputs with different resolutions are fused by the proposed new efficient Triple Attention (WLTA) blocks at four different levels from bottom to up. To demonstrate superior performance, WLTA-Net was first evaluated on public clinical datasets, then 3D Organoid datasets with Dice and PQ scores of 93.47±0.03 and 92.36±0.04, respectively.  The improved performance also comes with the added benefits of reduced complexity with 35 million parameters and lower computational cost in terms of GFLOPs. The code will be available upon acceptance. </Data></Cell>
    <Cell><Data ss:Type="String">Siyavash Shabani</Data></Cell>
    <Cell><Data ss:Type="String">sshabani@unr.edu</Data></Cell>
    <Cell><Data ss:Type="String">Siyavash Shabani (university of nevada, Reno)*; Sahar A  Mohammed (University of Nevada, reno ); Bahram  Parvin  (University of Nevada, Reno)</Data></Cell>
    <Cell><Data ss:Type="String">Shabani, Siyavash*; Mohammed, Sahar A ; Parvin , Bahram </Data></Cell>
    <Cell><Data ss:Type="String">sshabani@unr.edu*; sabulikailik@unr.edu; bparvin@unr.edu</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">unr.edu</Data></Cell>
    <Cell><Data ss:Type="Number">4</Data></Cell>
    <Cell><Data ss:Type="Number">4</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">MW_final.pdf (942,684 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Dong Yang (NVIDIA Corporation); Jianxu Chen (Leibniz-Institut für Analytische Wissenschaften – ISAS – e.V.); Jignesh Chowdary Gutta (Stony Brook University); Sharmishtaa Seshamani (Allen Institute)</Data></Cell>
    <Cell><Data ss:Type="String">dongy@nvidia.com; jxchen.ustc@gmail.com; jgutta@cs.stonybrook.edu; sharmishtaas@alleninstitute.org</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">Vision Transformers, 3D Medical Image Segmentation, Microscopy Images</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">23</Data></Cell>
    <Cell><Data ss:Type="String">3/17/2025 2:30:09 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">4/2/2025 6:51:05 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">Generalizable Unsupervised Microscopy Video Denoising via Weighted SpatioTemporal Sampling</Data></Cell>
    <Cell><Data ss:Type="String">Medical video denoising is essential for improving image quality and enhancing the reliability of clinical data. However, the limited availability of annotated datasets, the variability of noise patterns, and the absence of ground-truth reference frames present significant challenges for traditional supervised denoising approaches. On the other hand, current unsupervised video denoising methods often struggle to balance noise removal and motion preservation, leading to either excessive smoothing that degrades fine details or insufficient denoising that leaves residual noise. To address these challenges, we propose STS-UVD, a novel unsupervised video denoising (UVD) method that removes noise while preserving motion integrity. By refining the optical flow, our method ensures temporal consistency without compromising important motion details. Additionally, STS-UVD demonstrates strong generalization across different noise conditions and datasets, making it a robust solution for medical video analysis. Extensive experiments validate its effectiveness in enhancing video quality while maintaining structural and temporal coherence.</Data></Cell>
    <Cell><Data ss:Type="String">Nianyi Li</Data></Cell>
    <Cell><Data ss:Type="String">nianyil@clemson.edu</Data></Cell>
    <Cell><Data ss:Type="String">MARY AIYETIGBO (Clemson); Wanqi Yuan (Clemson); Feng Luo (Clemson University); Xin Li (University at Albany); Tong Ye (Clemson University); Nianyi Li (Clemson University)*</Data></Cell>
    <Cell><Data ss:Type="String">AIYETIGBO, MARY; Yuan, Wanqi; Luo, Feng; Li, Xin; Ye, Tong; Li, Nianyi*</Data></Cell>
    <Cell><Data ss:Type="String">maiyeti@clemson.edu; wanqiy@clemson.edu; luofeng@clemson.edu; xli48@albany.edu; ye7@clemson.edu; nianyil@clemson.edu*</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">clemson.edu; albany.edu</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025_23.pdf (2,320,814 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Jay Patravali (Microsoft); Ryoma  Bise (Kyushu University); Xubo Song  (Oregon Health and Science University)</Data></Cell>
    <Cell><Data ss:Type="String">jaypatravali@gmail.com; bise@ait.kyushu-u.ac.jp; songx@ohsu.edu</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">Medical Video Denoising; SpatialTemporal Sampling; Microscopy Imaging Denoising</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">25</Data></Cell>
    <Cell><Data ss:Type="String">3/17/2025 4:27:26 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">4/1/2025 9:04:53 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">Zero-Shot Denoising for Fluorescence Lifetime Imaging Microscopy with Intensity-Guided Learning</Data></Cell>
    <Cell><Data ss:Type="String">Multimodal and multi-information microscopy techniques such as Fluorescence Lifetime Imaging Microscopy (FLIM) extend the informational channels beyond intensity-based fluorescence microscopy but suffer from reduced image quality due to complex noise patterns. In FLIM, the intrinsic relationship between intensity and lifetime information means noise in each channel exhibits a multivariate dependence across channels without necessarily sharing structural features. Based on this, we present a novel Zero-Shot Denoising Framework with an Intensity-Guided Learning approach. Our correlation-preserving strategy maintains important biological information that might be lost when channels are processed independently. Our framework implements separate processing paths for each channel and utilizes a pre-trained intensity denoising prior to guide the refinement of lifetime components across multiple channels.  Through experiments on real-world FLIM-acquired biological samples, we show that our approach outperforms existing methods in both noise reduction and lifetime preservation, providing more reliable extraction of physiological and molecular information.</Data></Cell>
    <Cell><Data ss:Type="String">Hao Chen</Data></Cell>
    <Cell><Data ss:Type="String">hchen27@nd.edu</Data></Cell>
    <Cell><Data ss:Type="String">Hao Chen (University of Notre Dame)*; Julian Najera (University of Notre Dame); Dagmawit Geresu (University of Notre Dame	); Meenal Datta (University of Notre Dame); Cody Smith (University of Notre Dame); Scott Howard (	University of Notre Dame)</Data></Cell>
    <Cell><Data ss:Type="String">Chen, Hao*; Najera, Julian; Geresu, Dagmawit; Datta, Meenal; Smith, Cody; Howard, Scott</Data></Cell>
    <Cell><Data ss:Type="String">hchen27@nd.edu*; jnajera2@nd.edu; dgeresu@nd.edu; mdatta@nd.edu; csmith67@nd.edu; showard@nd.edu</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String">nd.edu</Data></Cell>
    <Cell><Data ss:Type="Number">4</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">0.75</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">CVPR2025__ZS_Denoising_FLIM.pdf (895,675 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Peng Liu (Boston college); Raphaël Marée (University of Liège); Shruthi Bannur (Microsoft Research); Weidong Cai (University of Sydney)</Data></Cell>
    <Cell><Data ss:Type="String">liupen@bc.edu; raphael.maree@uliege.be; shruthi.bannur@microsoft.com; tom.cai@sydney.edu.au</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">Multimodal Microscopy, Denoising, Fluorescence Lifetime Imaging Microscopy (FLIM), Zero-Shot Learning</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">31</Data></Cell>
    <Cell><Data ss:Type="String">3/17/2025 5:31:11 PM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">3/30/2025 6:04:02 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">Low-Frame-Rate Cell Tracking: Unmet Needs and Future Directions</Data></Cell>
    <Cell><Data ss:Type="String">Deep learning has significantly improved cell tracking, but most methods rely on high-frame-rate imaging, which can cause photobleaching and phototoxicity. Low-frame-rate imaging mitigates these effects, enabling long-term observations while introducing new tracking challenges. This work presents a novel dataset for low-frame-rate cell tracking, featuring long-term image sequences with ground-truth annotations for cell identification, mitosis, and movement. We sample well-established tracking methods and reveal their limitations in handling sparse temporal data. Our analysis highlights key challenges, including increased cell displacement, missed mitosis events, and tracking ambiguities. Moreover, we discuss how to set up an objective framework for cell tracking and future directions to develop robust tracking methods tailored to low-frame-rate conditions.</Data></Cell>
    <Cell><Data ss:Type="String">Mina Gachloo</Data></Cell>
    <Cell><Data ss:Type="String">mgachlo@clemson.edu</Data></Cell>
    <Cell><Data ss:Type="String">Mina Gachloo (Clemson University )*; Akhila  Nangineedi  (Clemson University	); Mahsa Partovi (Clemson University	); Fardifa Fathmiul Alam (Clemson University	); Tzu Yu  (Clemson University	); James Schvaneveldt (Clemson University	); Xiaoming Lu (Clemson University	); Tirthankar  Biswas (Clemson University	); Marc Russel Birtwistle (Clemson University	); Federico Iuricich (Clemson University	)</Data></Cell>
    <Cell><Data ss:Type="String">Gachloo, Mina*; Nangineedi , Akhila ; Partovi, Mahsa; Fathmiul Alam, Fardifa; Yu , Tzu; Schvaneveldt, James; Lu, Xiaoming; Biswas, Tirthankar ; Birtwistle, Marc Russel; Iuricich, Federico</Data></Cell>
    <Cell><Data ss:Type="String">mgachlo@clemson.edu*; anangin@clemson.edu; mpartov@clemson.edu; fardifa@g.clemson.edu; tzuyuc@clemson.edu; jschvan@clemson.edu; xiaomin@clemson.edu; tbiswas@clemson.edu; mbirtwi@clemson.edu; fiurici@clemson.edu</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">clemson.edu</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">CVPR_Workshop_2025.pdf (1,687,426 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Peter Bajcsy (NIST); Ryoma  Bise (Kyushu University); Weidong Cai (University of Sydney)</Data></Cell>
    <Cell><Data ss:Type="String">peter.bajcsy@nist.gov; bise@ait.kyushu-u.ac.jp; tom.cai@sydney.edu.au</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">cell tracking,&#10;low-frame-rate acquisition,&#10;benchmark,&#10;annotation, &#10;microscopy image dataset</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">34</Data></Cell>
    <Cell><Data ss:Type="String">3/17/2025 10:20:57 PM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">3/31/2025 4:25:39 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">IAUNet: Instance-Aware U-Net</Data></Cell>
    <Cell><Data ss:Type="String">Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. The dataset will be made public upon acceptance. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://anonymous.4open.science/r/IAUNet-EA9B</Data></Cell>
    <Cell><Data ss:Type="String">Yaroslav Prytula</Data></Cell>
    <Cell><Data ss:Type="String">yaroslav.prytula@ut.ee</Data></Cell>
    <Cell><Data ss:Type="String">Yaroslav Prytula (University of Tartu)*; Illia Tsiporenko (University of Tartu); Ali Zeynalli (University of Tartu); Dmytro Fishman (University of Tartu)</Data></Cell>
    <Cell><Data ss:Type="String">Prytula, Yaroslav*; Tsiporenko, Illia; Zeynalli, Ali; Fishman, Dmytro</Data></Cell>
    <Cell><Data ss:Type="String">yaroslav.prytula@ut.ee*; illia.tsiporenko@ut.ee; ali.zeynalli@ut.ee; dmytro.fishman@ut.ee</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">ut.ee; ucu.edu.ua</Data></Cell>
    <Cell><Data ss:Type="Number">4</Data></Cell>
    <Cell><Data ss:Type="Number">4</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">IAUNet_WCVPR@25.pdf (9,880,558 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Arnav Bhavsar (IIT Mandi); Hang Chang (Lawrence Berkeley National Laboratory); Vedrana Dahl (Technical University of Denmark); Weidong Cai (University of Sydney)</Data></Cell>
    <Cell><Data ss:Type="String">arnav@iitmandi.ac.in; hchang@lbl.gov; vand@dtu.dk; tom.cai@sydney.edu.au</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">Medical and Biological Vision, Cell Microscopy, Instance Segmentation, Deep Learning</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">36</Data></Cell>
    <Cell><Data ss:Type="String">3/18/2025 5:13:03 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">3/31/2025 4:22:43 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">CytoFM: The first cytology foundation model</Data></Cell>
    <Cell><Data ss:Type="String">Cytology is essential for cancer diagnostics and screening due to its minimally invasive nature. However, the development of robust deep learning models for digital cytology is challenging due to the heterogeneity in staining and preparation methods of samples, differences across organs, and the limited availability of large, diverse, annotated datasets. Developing a task-specific model for every cytology application is impractical and non-cytology-specific foundation models struggle to generalize to tasks in this domain where the emphasis is on cell morphology. To address these challenges, we introduce CytoFM, the first cytology self-supervised foundation model. Using iBOT, a self-supervised Vision Transformer (ViT) training framework incorporating masked image modeling and self-distillation, we pretrain CytoFM on a diverse collection of cytology datasets to learn robust, transferable representations. We evaluate CytoFM on multiple downstream cytology tasks, including breast cancer classification and cell type identification, using an attention-based multiple instance learning (ABMIL) framework. Our results demonstrate that CytoFM performs better on two out of three downstream tasks than existing foundation models pretrained on histopathology (UNI) or natural images (iBOT-Imagenet). Visualizations of learned representations demonstrate our model is able to attend to cytologically relevant features. Despite a small pre-training dataset, CytoFM's promising results highlight the ability of task-agnostic pre-training approaches to learn robust and generalizable features from cytology data.</Data></Cell>
    <Cell><Data ss:Type="String">Vedrana Ivezic</Data></Cell>
    <Cell><Data ss:Type="String">vivezic@g.ucla.edu</Data></Cell>
    <Cell><Data ss:Type="String">Vedrana Ivezic (UCLA)*; Ashwath  Radhachandran (UCLA); Ekaterina Redekop (UCLA); Shreeram Athreya (UCLA); Dongwoo Lee (UCLA); Vivek Sant (UT Southwestern); Corey Arnold (UCLA); William Speier (UCLA)</Data></Cell>
    <Cell><Data ss:Type="String">Ivezic, Vedrana*; Radhachandran, Ashwath ; Redekop, Ekaterina; Athreya, Shreeram; Lee, Dongwoo; Sant, Vivek; Arnold, Corey; Speier, William</Data></Cell>
    <Cell><Data ss:Type="String">vivezic@g.ucla.edu*; ashwathradha123@g.ucla.edu; eredekop@g.ucla.edu; shreeram@ucla.edu; rlee8517@g.ucla.edu; vivek.sant@utsouthwestern.edu; cwarnold@ucla.edu; Speier@ucla.edu</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">ucla.edu</Data></Cell>
    <Cell><Data ss:Type="Number">4</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">0.75</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">cytofm_cvpr_25_final_version.pdf (1,053,659 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Raphaël Marée (University of Liège); Xubo Song  (Oregon Health and Science University); Yang Song (University of New South Wales); Young Min Shin (Noul Co., Ltd.)</Data></Cell>
    <Cell><Data ss:Type="String">raphael.maree@uliege.be; songx@ohsu.edu; yang.song1@unsw.edu.au; young@noul.com</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">Cytology, foundation model, self-supervised</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="Number">37</Data></Cell>
    <Cell><Data ss:Type="String">3/18/2025 6:02:34 AM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">3/31/2025 5:22:02 PM +00:00</Data></Cell>
    <Cell><Data ss:Type="String">Beyond Neurofibrillary Tangles: Explainable AI for Microscopic Tauopathy Classification in Immunofluorescence Imaging</Data></Cell>
    <Cell><Data ss:Type="String">The immunodetection of aberrant molecular changes in Tau polypeptide plays a critical role in the diagnosis and pathogenesis of tauopathies such as Alzheimer's Disease (AD), Progressive Supranuclear Palsy (PSP), and Frontotemporal Dementia (FTD). However, histopathological evaluations of immunofluorescence micrographs remain subjective and labor-intensive. This study presents a deep learning framework for multi-class classification of tauopathies using immunofluorescence microscopy. Five Convolutional Neural Networks (CNNs)—ResNet50, ResNet101, DenseNet121, EfficientNet-B0, and Res2Net—are evaluated with explainability techniques, including EigenGradCAM, AblationCAM, FullGrad, LIME, and Deep Feature Factorization.&#10;&#10;Our findings indicate that high-performing models, particularly EfficientNet-B0 and DenseNet121, frequently emphasize background regions rather than neurofibrillary tangles (NFTs). Ablation studies reveal that removing background features affects classification as significantly as NFT removal, suggesting that secondary microstructural events outside classical tau markers may contribute to model predictions. Lower-performing models relied more strictly on NFTs, potentially reflecting a constrained feature extraction strategy.&#10;&#10;These results highlight AI’s potential in pathology analysis to identify structural biomarkers beyond conventional Tau-based markers. Further validation is required to determine whether background saliency represents meaningful pathology or dataset artifacts. Future work should focus on multi-modal integration and biomarker validation to enhance AI’s clinical utility in neuropathology.</Data></Cell>
    <Cell><Data ss:Type="String">Jesus Dassaef López-Barrios</Data></Cell>
    <Cell><Data ss:Type="String">a01366815@tec.mx</Data></Cell>
    <Cell><Data ss:Type="String">Jesus Dassaef López-Barrios (Tecnologico de Monterrey)*; Miguel Angel Ontiveros-Torres (Tecnologico de Monterrey); Jose Antonio Cantoral-Ceballos (Tecnologico de Monterrey)</Data></Cell>
    <Cell><Data ss:Type="String">López-Barrios, Jesus Dassaef*; Ontiveros-Torres, Miguel Angel; Cantoral-Ceballos, Jose Antonio</Data></Cell>
    <Cell><Data ss:Type="String">a01366815@tec.mx*; miguelontiveros@tec.mx; joseantonio.cantoral@tec.mx</Data></Cell>
    <Cell><Data ss:Type="String">CVMI2025</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">tec.mx; itesm.mx</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell><Data ss:Type="Number">3</Data></Cell>
    <Cell ss:StyleID="s63"><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Disabled (0)</Data></Cell>
    <Cell><Data ss:Type="String">Accept</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">No</Data></Cell>
    <Cell><Data ss:Type="String">ID37_beyond_nfts_xai_tau_class.pdf (7,619,667 bytes)</Data></Cell>
    <Cell><Data ss:Type="Number">1</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="Number">0</Data></Cell>
    <Cell><Data ss:Type="String">Jamalia  Sultana  (Stony Brook University); Mohammad Haft-Javaherian (MIT &amp; Harvard Medical School); Tong Zhang (Peng Cheng Labortory)</Data></Cell>
    <Cell><Data ss:Type="String">jsultana@cs.stonybrook.edu; haft@mit.edu; zhangt02@pcl.ac.cn</Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String"></Data></Cell>
    <Cell><Data ss:Type="String">Agreement accepted</Data></Cell>
    <Cell><Data ss:Type="String">Tauopathy Classification&#10;Explainable AI (XAI)&#10;Immunofluorescence Microscopy&#10;Neurofibrillary Tangles (NFTs)&#10;Convolutional Neural Networks (CNNs)&#10;Deep Learning&#10;Alzheimer’s Disease (AD)&#10;Progressive Supranuclear Palsy (PSP)&#10;Frontotemporal Dementia (FTD)&#10;Medical Image Analysis&#10;Neuropathology Biomarkers</Data></Cell>
   </Row>
  </Table>
  <WorksheetOptions xmlns="urn:schemas-microsoft-com:office:excel">
   <Selected/>
   <ProtectObjects>False</ProtectObjects>
   <ProtectScenarios>False</ProtectScenarios>
  </WorksheetOptions>
 </Worksheet>
</Workbook>
